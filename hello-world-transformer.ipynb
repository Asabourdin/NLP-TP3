{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a616de03",
   "metadata": {},
   "source": [
    "# Hello world transformers ✨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb7291",
   "metadata": {},
   "source": [
    "In this notebook we will explore the basics of the Hugging Face library by using a pre-trained model to classify text. Here is our exemple text with which we will test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d7615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
    "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
    "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1ab08",
   "metadata": {},
   "source": [
    "## Question 1: Understanding Pipelines\n",
    "### Question 1.1 — What is a pipeline in Hugging Face Transformers? What does it abstract away from the user?\n",
    "\n",
    "A **pipeline** in Hugging Face Transformers is a high-level API that provides an easy way to use pre-trained models for common NLP (and multimodal) tasks.\n",
    "\n",
    "It abstracts away many technical details, including:\n",
    "- Loading the correct **pre-trained model** for a given task\n",
    "- Loading and applying the appropriate **tokenizer**\n",
    "- Handling **preprocessing** (tokenization, padding, truncation)\n",
    "- Running **inference** with the model\n",
    "- Applying **postprocessing** to convert raw model outputs into human-readable results (labels, scores, generated text, etc.)\n",
    "\n",
    "This allows users to perform complex tasks with just a few lines of code, without needing to understand the low-level model architecture or training details.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1.2 — List at least 3 other tasks available in pipelines (besides text-classification)\n",
    "\n",
    "Besides `text-classification`, Hugging Face pipelines support many other tasks. Examples include:\n",
    "\n",
    "- **sentiment-analysis** – classify text sentiment (positive/negative, etc.)\n",
    "- **question-answering** – extract answers from a context given a question\n",
    "- **text-generation** – generate text using language models (e.g., GPT-style models)\n",
    "- **summarization** – generate concise summaries of long texts\n",
    "- **translation** – translate text between languages\n",
    "- **named-entity-recognition** – identify entities such as names, locations, and organizations\n",
    "- **fill-mask** – predict missing words in masked sentences\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1.3 — What happens when you don’t specify a model? How can you specify a specific model?\n",
    "\n",
    "When you do **not** specify a model in a pipeline, Hugging Face automatically loads a **default pre-trained model** that is considered suitable for the chosen task.\n",
    "\n",
    "For example:\n",
    "- `pipeline(\"text-classification\")` loads a default sentiment-analysis model.\n",
    "\n",
    "To specify a **specific model**, you can pass its name explicitly using the `model` argument:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d00f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19fedbe",
   "metadata": {},
   "source": [
    "## Question 2 — Text Classification Deep Dive\n",
    "\n",
    "### Question 2.1 What is the default model used for text-classification?\n",
    "\n",
    "When you run a Hugging Face `pipeline(\"text-classification\")` **without specifying a model**, it automatically loads a default pre-trained model.  \n",
    "In most recent versions of the Transformers library, this default is:\n",
    "\n",
    "**`distilbert-base-uncased-finetuned-sst-2-english`** —  \n",
    "a DistilBERT model fine-tuned specifically for sentiment classification. :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2.2 What dataset was this model fine-tuned on? What kind of text does it work best with?\n",
    "\n",
    "The model `distilbert-base-uncased-finetuned-sst-2-english` was fine-tuned on the **Stanford Sentiment Treebank (SST-2)** dataset, a benchmark dataset for sentiment analysis.  \n",
    "This dataset consists of English movie reviews labeled as **positive** or **negative** sentiment, and the model excels at classifying similar sentiment tasks on short to medium-length English text. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "So it works best with **general English text where sentiment (positive/negative) is clear**, such as reviews, tweets, or customer feedback.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2.3 What does the score field represent? What range of values can it have?\n",
    "\n",
    "In the output of the classification pipeline, the `\"score\"` field represents the model’s **confidence (probability)** in its prediction for that label.  \n",
    "- It is the **softmax probability** assigned to the predicted label.\n",
    "- The value ranges from **0.0 to 1.0**.\n",
    "- A **higher score** means the model is more confident in that label.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "[{'label': 'NEGATIVE', 'score': 0.95},\n",
    " {'label': 'POSITIVE', 'score': 0.05}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d44b03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.901546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.901546"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c4a69",
   "metadata": {},
   "source": [
    "## Question 3 — Named Entity Recognition (NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a669309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.879009</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>Germany</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.556567</td>\n",
       "      <td>Mega</td>\n",
       "      <td>208</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.590258</td>\n",
       "      <td>##tron</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.669693</td>\n",
       "      <td>Decept</td>\n",
       "      <td>253</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.498349</td>\n",
       "      <td>##icons</td>\n",
       "      <td>259</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.775361</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.879009         Amazon      5   11\n",
       "1         MISC  0.990859  Optimus Prime     36   49\n",
       "2          LOC  0.999755        Germany     90   97\n",
       "3         MISC  0.556567           Mega    208  212\n",
       "4          PER  0.590258         ##tron    212  216\n",
       "5          ORG  0.669693         Decept    253  259\n",
       "6         MISC  0.498349        ##icons    259  264\n",
       "7         MISC  0.775361       Megatron    350  358\n",
       "8         MISC  0.987854  Optimus Prime    367  380\n",
       "9          PER  0.812096      Bumblebee    502  511"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58faa38e",
   "metadata": {},
   "source": [
    "\n",
    "### Question 3.1 What does `aggregation_strategy=\"simple\"` do in the NER pipeline?\n",
    "\n",
    "In a Hugging Face NER (token-classification) pipeline, `aggregation_strategy=\"simple\"` groups together consecutive tokens that belong to the **same entity type** into a single entity span.\n",
    "\n",
    "Without aggregation, the model outputs predictions **per token**, which can be difficult to interpret when words are split into sub-tokens.  \n",
    "With `\"simple\"` aggregation:\n",
    "- sub-tokens belonging to the same entity are merged,\n",
    "- the entity label is assigned to the whole word or phrase,\n",
    "- a single confidence score is computed for the aggregated entity.\n",
    "\n",
    "This makes the output more readable and closer to how humans think about named entities.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3.2 What do the entity types mean? (ORG, MISC, LOC, PER)\n",
    "\n",
    "The entity types come from the **CoNLL-2003 annotation scheme**:\n",
    "\n",
    "- **PER**: Person  \n",
    "  → names of people (e.g. *Bumblebee*)\n",
    "- **ORG**: Organization  \n",
    "  → companies, institutions, groups (e.g. *Amazon*)\n",
    "- **LOC**: Location  \n",
    "  → geographical locations such as cities, countries, regions\n",
    "- **MISC**: Miscellaneous  \n",
    "  → entities that do not fit the other categories, such as products, events, nationalities, or fictional groups\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3.3 Why do some words appear with a `##` prefix (e.g. `##tron`, `##icons`)?\n",
    "\n",
    "The `##` prefix indicates that the tokenizer uses **subword tokenization** (specifically WordPiece tokenization).\n",
    "\n",
    "- A token **without** `##` marks the **start of a word**.\n",
    "- A token **with** `##` means “this piece continues the previous token”.\n",
    "\n",
    "For example:\n",
    "- `Megatron` → `Mega` + `##tron`\n",
    "- `Decepticons` → `Decept` + `##icons`\n",
    "\n",
    "This allows the model to handle:\n",
    "- rare or unknown words,\n",
    "- new words formed from known subwords,\n",
    "- large vocabularies efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3.4 Why were \"Megatron\" and \"Decepticons\" split incorrectly? What does this say about the training data?\n",
    "\n",
    "The model splits these words because they are **unlikely to appear frequently (or at all)** in the training data.\n",
    "\n",
    "Reasons:\n",
    "- They are **fictional names** from popular culture.\n",
    "- The model was trained mainly on **news articles**, not on movie or toy-related text.\n",
    "- As a result, these words are treated as unknown and broken into subwords that *do* exist in the vocabulary.\n",
    "\n",
    "This tells us that:\n",
    "- NER models perform best on text **similar to their training domain**.\n",
    "- Out-of-domain entities (fictional characters, slang, brand-new names) are harder to recognize correctly.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3.5 What is the CoNLL-2003 dataset?\n",
    "\n",
    "The **CoNLL-2003** dataset is a standard benchmark dataset for Named Entity Recognition.\n",
    "\n",
    "It consists of:\n",
    "- English newswire articles (from Reuters)\n",
    "- Manually annotated named entities\n",
    "- Four entity types: **PER, ORG, LOC, MISC**\n",
    "\n",
    "The model  \n",
    "**`dbmdz/bert-large-cased-finetuned-conll03-english`**  \n",
    "is a BERT model fine-tuned specifically on this dataset, which explains why it performs well on **formal English news text**.\n",
    "\n",
    "---\n",
    "\n",
    "### How might the choice of tokenizer affect NER performance?\n",
    "\n",
    "The tokenizer directly affects how text is split into tokens, which in turn affects entity recognition.\n",
    "\n",
    "- **Over-splitting words** can make it harder to correctly assign entity labels.\n",
    "- **Cased tokenizers** (that preserve uppercase letters) usually perform better for NER, since capitalization carries important information (e.g. names).\n",
    "- Tokenizers trained on **domain-specific data** (medical, legal, social media) can significantly improve NER performance in those domains.\n",
    "\n",
    "In short, a tokenizer that better matches the **language style and vocabulary** of the task will usually lead to better NER results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61dc70",
   "metadata": {},
   "source": [
    "## Question 4 — Question Answering Systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb38f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631292</td>\n",
       "      <td>335</td>\n",
       "      <td>358</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                   answer\n",
       "0  0.631292    335  358  an exchange of Megatron"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pipeline(\"question-answering\")\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba83dfb",
   "metadata": {},
   "source": [
    "### Question 4.1 What type of question answering is this? (Extractive vs. Generative)\n",
    "\n",
    "This is **extractive question answering**.\n",
    "\n",
    "In extractive QA, the model:\n",
    "- is given a **context** (a passage of text),\n",
    "- selects a **span of text directly from that context** as the answer.\n",
    "\n",
    "It does **not generate new text**; it only extracts what already exists in the input.  \n",
    "This is exactly how the Hugging Face `question-answering` pipeline works by default.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.2 What do the start and end indices represent? Why are they important?\n",
    "\n",
    "The `start` and `end` indices represent:\n",
    "- the **character positions** in the input context\n",
    "- that delimit the extracted answer span.\n",
    "\n",
    "They are important because:\n",
    "- they specify *where* in the original text the answer was found,\n",
    "- they allow precise extraction of the answer,\n",
    "- they make the model’s decision **traceable and interpretable**.\n",
    "\n",
    "In extractive QA, predicting the correct start and end positions is the core learning task.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.3 What is the SQuAD dataset?\n",
    "\n",
    "**SQuAD (Stanford Question Answering Dataset)** is a benchmark dataset for extractive QA.\n",
    "\n",
    "It consists of:\n",
    "- Wikipedia passages (contexts),\n",
    "- human-written questions,\n",
    "- answers that are **exact spans from the context**.\n",
    "\n",
    "The model  \n",
    "`distilbert-base-cased-distilled-squad`  \n",
    "is a DistilBERT model fine-tuned on SQuAD, meaning it is optimized to:\n",
    "- read factual text,\n",
    "- locate short, precise answers within a passage.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.4 Try to think of a question this model CANNOT answer. Why would it fail?\n",
    "\n",
    "Example of a question the model cannot answer:\n",
    "\n",
    "> **\"Why does Bumblebee dislike the Decepticons?\"**\n",
    "\n",
    "It would fail because:\n",
    "- the answer requires **reasoning and background knowledge**,\n",
    "- the context does not explicitly state the reason,\n",
    "- extractive models cannot infer or invent explanations.\n",
    "\n",
    "Another failing example:\n",
    "\n",
    "> **\"What happened after Amazon replied?\"**\n",
    "\n",
    "This information is **not present in the text**, so the model has nothing to extract.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.5 Challenge — Difference between extractive and generative QA\n",
    "\n",
    "| Extractive QA | Generative QA |\n",
    "|---------------|---------------|\n",
    "| Selects answers directly from the context | Generates new answers in natural language |\n",
    "| Answer must exist verbatim in the text | Answer may not appear in the text |\n",
    "| Uses start/end span prediction | Uses text generation |\n",
    "| More reliable and factual | More flexible but can hallucinate |\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.6 Example of a generative QA model\n",
    "\n",
    "An example of a **generative QA model** on Hugging Face is:\n",
    "\n",
    "- `google/flan-t5-base`\n",
    "\n",
    "This model can:\n",
    "- reason over text,\n",
    "- generate full-sentence answers,\n",
    "- answer questions even if the exact wording is not in the context.\n",
    "\n",
    "---\n",
    "\n",
    "### Asking questions the extractive model cannot answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "898fe5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631292</td>\n",
       "      <td>335</td>\n",
       "      <td>358</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.151185</td>\n",
       "      <td>266</td>\n",
       "      <td>302</td>\n",
       "      <td>I hope you can understand my dilemma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                                answer\n",
       "0  0.631292    335  358               an exchange of Megatron\n",
       "1  0.151185    266  302  I hope you can understand my dilemma"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "reader = pipeline(\"question-answering\")\n",
    "\n",
    "# A question answerable from the text\n",
    "question_ok = \"What does the customer want?\"\n",
    "out_ok = reader(question=question_ok, context=text)\n",
    "\n",
    "# A question that requires reasoning or external knowledge\n",
    "question_fail = \"Why does Bumblebee hate the Decepticons?\"\n",
    "out_fail = reader(question=question_fail, context=text)\n",
    "\n",
    "pd.DataFrame([out_ok, out_fail])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9e986",
   "metadata": {},
   "source": [
    "We see that the model always returns a span of text, even when the question cannot truly be answered from the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25319f",
   "metadata": {},
   "source": [
    "## Question 5 — Text Summarization\n",
    "\n",
    "### Question 5.1 What is the difference between extractive and abstractive summarization?\n",
    "\n",
    "- **Extractive summarization** selects sentences or phrases **directly from the original text** and concatenates them to form a summary.  \n",
    "  The summary only contains text that already exists in the document and does not generate new wording.\n",
    "\n",
    "- **Abstractive summarization** generates a **new summary in natural language** by paraphrasing, compressing, or reformulating the original content.  \n",
    "  The summary may use words or sentence structures that do not appear in the source text.\n",
    "\n",
    "The Hugging Face `summarization` pipeline relies on **abstractive summarization models**.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5.2 What is the default model used for summarization?\n",
    "\n",
    "When the summarization pipeline is used without specifying a model, the default model is typically:\n",
    "\n",
    "- **facebook/bart-large-cnn**\n",
    "\n",
    "From the Hugging Face Model Hub:\n",
    "\n",
    "- The model is **abstractive**\n",
    "- It uses the **BART (Bidirectional and Auto-Regressive Transformers)** architecture\n",
    "- It follows an **encoder–decoder (sequence-to-sequence)** design\n",
    "- It was trained on the **CNN/DailyMail** dataset, composed of news articles and human-written summaries\n",
    "\n",
    "This makes the model particularly effective for **news-style summarization**.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5.3 What do the `max_length` and `min_length` parameters control? What happens if `min_length > max_length`?\n",
    "\n",
    "- **max_length** defines the maximum number of tokens allowed in the generated summary  \n",
    "- **min_length** defines the minimum number of tokens the summary must contain  \n",
    "\n",
    "These parameters control the level of **compression versus detail** in the summary.\n",
    "\n",
    "If `min_length` is greater than `max_length`, text generation fails because the constraints are inconsistent and cannot be satisfied simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5.4 What does `clean_up_tokenization_spaces=True` do? Why is it useful for summarization?\n",
    "\n",
    "This parameter removes tokenization artifacts such as:\n",
    "- unnecessary spaces before punctuation\n",
    "- awkward spacing between words\n",
    "\n",
    "It is useful for summarization because summaries are intended to be **human-readable**, and cleaning up spacing improves readability and grammatical quality.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5.5 Challenge — Two different summarization models on the Hub\n",
    "\n",
    "- **Model optimized for short texts (e.g. news articles):**\n",
    "  - **facebook/bart-large-cnn**\n",
    "  - Architecture: BART (encoder–decoder, sequence-to-sequence)\n",
    "  - Training data: CNN/DailyMail\n",
    "  - Best suited for short to medium-length news content\n",
    "\n",
    "- **Model that can handle longer documents:**\n",
    "  - **google/pegasus-arxiv** (or google/pegasus-pubmed)\n",
    "  - Architecture: PEGASUS (encoder–decoder, sequence-to-sequence)\n",
    "  - Training data: ArXiv research papers (or PubMed biomedical articles)\n",
    "  - Designed for long, structured documents\n",
    "\n",
    "**Comparison:**  \n",
    "Both models use encoder–decoder architectures. BART is optimized for journalistic text, while PEGASUS is trained on much longer documents and better captures long-range dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### Why might summarization be more challenging than text classification? What linguistic capabilities does the model need?\n",
    "\n",
    "Summarization is more challenging because it requires the model to:\n",
    "- understand the **global meaning** of a document\n",
    "- identify **important versus secondary information**\n",
    "- model discourse structure and coherence\n",
    "- paraphrase and compress content\n",
    "- generate fluent and grammatically correct natural language\n",
    "\n",
    "Text classification only assigns a label, whereas summarization requires **deep understanding and language generation**, making it a significantly more complex task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac3637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "Your min_length=56 must be inferior than your max_length=45.\n",
      "/Users/alienorsabourdin/NLP-TP2/NLP-TP3/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1633: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (45). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
