{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a616de03",
   "metadata": {},
   "source": [
    "# Hello world transformers ✨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb7291",
   "metadata": {},
   "source": [
    "In this notebook we will explore the basics of the Hugging Face library by using a pre-trained model to classify text. Here is our exemple text with which we will test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d7615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
    "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
    "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1ab08",
   "metadata": {},
   "source": [
    "## Question 1: Understanding Pipelines\n",
    "### Question 1.1 — What is a pipeline in Hugging Face Transformers? What does it abstract away from the user?\n",
    "\n",
    "A **pipeline** in Hugging Face Transformers is a high-level API that provides an easy way to use pre-trained models for common NLP (and multimodal) tasks.\n",
    "\n",
    "It abstracts away many technical details, including:\n",
    "- Loading the correct **pre-trained model** for a given task\n",
    "- Loading and applying the appropriate **tokenizer**\n",
    "- Handling **preprocessing** (tokenization, padding, truncation)\n",
    "- Running **inference** with the model\n",
    "- Applying **postprocessing** to convert raw model outputs into human-readable results (labels, scores, generated text, etc.)\n",
    "\n",
    "This allows users to perform complex tasks with just a few lines of code, without needing to understand the low-level model architecture or training details.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1.2 — List at least 3 other tasks available in pipelines (besides text-classification)\n",
    "\n",
    "Besides `text-classification`, Hugging Face pipelines support many other tasks. Examples include:\n",
    "\n",
    "- **sentiment-analysis** – classify text sentiment (positive/negative, etc.)\n",
    "- **question-answering** – extract answers from a context given a question\n",
    "- **text-generation** – generate text using language models (e.g., GPT-style models)\n",
    "- **summarization** – generate concise summaries of long texts\n",
    "- **translation** – translate text between languages\n",
    "- **named-entity-recognition** – identify entities such as names, locations, and organizations\n",
    "- **fill-mask** – predict missing words in masked sentences\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1.3 — What happens when you don’t specify a model? How can you specify a specific model?\n",
    "\n",
    "When you do **not** specify a model in a pipeline, Hugging Face automatically loads a **default pre-trained model** that is considered suitable for the chosen task.\n",
    "\n",
    "For example:\n",
    "- `pipeline(\"text-classification\")` loads a default sentiment-analysis model.\n",
    "\n",
    "To specify a **specific model**, you can pass its name explicitly using the `model` argument:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d00f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alienorsabourdin/NLP-TP2/NLP-TP3/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19fedbe",
   "metadata": {},
   "source": [
    "## Question 2 — Text Classification Deep Dive\n",
    "\n",
    "### Question 2.1 What is the default model used for text-classification?\n",
    "\n",
    "When you run a Hugging Face `pipeline(\"text-classification\")` **without specifying a model**, it automatically loads a default pre-trained model.  \n",
    "In most recent versions of the Transformers library, this default is:\n",
    "\n",
    "**`distilbert-base-uncased-finetuned-sst-2-english`** —  \n",
    "a DistilBERT model fine-tuned specifically for sentiment classification. :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2.2 What dataset was this model fine-tuned on? What kind of text does it work best with?\n",
    "\n",
    "The model `distilbert-base-uncased-finetuned-sst-2-english` was fine-tuned on the **Stanford Sentiment Treebank (SST-2)** dataset, a benchmark dataset for sentiment analysis.  \n",
    "This dataset consists of English movie reviews labeled as **positive** or **negative** sentiment, and the model excels at classifying similar sentiment tasks on short to medium-length English text. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "So it works best with **general English text where sentiment (positive/negative) is clear**, such as reviews, tweets, or customer feedback.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2.3 What does the score field represent? What range of values can it have?\n",
    "\n",
    "In the output of the classification pipeline, the `\"score\"` field represents the model’s **confidence (probability)** in its prediction for that label.  \n",
    "- It is the **softmax probability** assigned to the predicted label.\n",
    "- The value ranges from **0.0 to 1.0**.\n",
    "- A **higher score** means the model is more confident in that label.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "[{'label': 'NEGATIVE', 'score': 0.95},\n",
    " {'label': 'POSITIVE', 'score': 0.05}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d44b03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.901546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.901546"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c4a69",
   "metadata": {},
   "source": [
    "## Question 3 — Named Entity Recognition (NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a669309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.879009</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>Germany</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.556567</td>\n",
       "      <td>Mega</td>\n",
       "      <td>208</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.590258</td>\n",
       "      <td>##tron</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.669693</td>\n",
       "      <td>Decept</td>\n",
       "      <td>253</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.498349</td>\n",
       "      <td>##icons</td>\n",
       "      <td>259</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.775361</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.879009         Amazon      5   11\n",
       "1         MISC  0.990859  Optimus Prime     36   49\n",
       "2          LOC  0.999755        Germany     90   97\n",
       "3         MISC  0.556567           Mega    208  212\n",
       "4          PER  0.590258         ##tron    212  216\n",
       "5          ORG  0.669693         Decept    253  259\n",
       "6         MISC  0.498349        ##icons    259  264\n",
       "7         MISC  0.775361       Megatron    350  358\n",
       "8         MISC  0.987854  Optimus Prime    367  380\n",
       "9          PER  0.812096      Bumblebee    502  511"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58faa38e",
   "metadata": {},
   "source": [
    "\n",
    "### Question 3.1 What does `aggregation_strategy=\"simple\"` do in the NER pipeline?\n",
    "\n",
    "In a Hugging Face NER (token-classification) pipeline, `aggregation_strategy=\"simple\"` groups together consecutive tokens that belong to the **same entity type** into a single entity span.\n",
    "\n",
    "Without aggregation, the model outputs predictions **per token**, which can be difficult to interpret when words are split into sub-tokens.  \n",
    "With `\"simple\"` aggregation:\n",
    "- sub-tokens belonging to the same entity are merged,\n",
    "- the entity label is assigned to the whole word or phrase,\n",
    "- a single confidence score is computed for the aggregated entity.\n",
    "\n",
    "This makes the output more readable and closer to how humans think about named entities.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3.2 What do the entity types mean? (ORG, MISC, LOC, PER)\n",
    "\n",
    "The entity types come from the **CoNLL-2003 annotation scheme**:\n",
    "\n",
    "- **PER**: Person  \n",
    "  → names of people (e.g. *Bumblebee*)\n",
    "- **ORG**: Organization  \n",
    "  → companies, institutions, groups (e.g. *Amazon*)\n",
    "- **LOC**: Location  \n",
    "  → geographical locations such as cities, countries, regions\n",
    "- **MISC**: Miscellaneous  \n",
    "  → entities that do not fit the other categories, such as products, events, nationalities, or fictional groups\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3.3 Why do some words appear with a `##` prefix (e.g. `##tron`, `##icons`)?\n",
    "\n",
    "The `##` prefix indicates that the tokenizer uses **subword tokenization** (specifically WordPiece tokenization).\n",
    "\n",
    "- A token **without** `##` marks the **start of a word**.\n",
    "- A token **with** `##` means “this piece continues the previous token”.\n",
    "\n",
    "For example:\n",
    "- `Megatron` → `Mega` + `##tron`\n",
    "- `Decepticons` → `Decept` + `##icons`\n",
    "\n",
    "This allows the model to handle:\n",
    "- rare or unknown words,\n",
    "- new words formed from known subwords,\n",
    "- large vocabularies efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3.4 Why were \"Megatron\" and \"Decepticons\" split incorrectly? What does this say about the training data?\n",
    "\n",
    "The model splits these words because they are **unlikely to appear frequently (or at all)** in the training data.\n",
    "\n",
    "Reasons:\n",
    "- They are **fictional names** from popular culture.\n",
    "- The model was trained mainly on **news articles**, not on movie or toy-related text.\n",
    "- As a result, these words are treated as unknown and broken into subwords that *do* exist in the vocabulary.\n",
    "\n",
    "This tells us that:\n",
    "- NER models perform best on text **similar to their training domain**.\n",
    "- Out-of-domain entities (fictional characters, slang, brand-new names) are harder to recognize correctly.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3.5 What is the CoNLL-2003 dataset?\n",
    "\n",
    "The **CoNLL-2003** dataset is a standard benchmark dataset for Named Entity Recognition.\n",
    "\n",
    "It consists of:\n",
    "- English newswire articles (from Reuters)\n",
    "- Manually annotated named entities\n",
    "- Four entity types: **PER, ORG, LOC, MISC**\n",
    "\n",
    "The model  \n",
    "**`dbmdz/bert-large-cased-finetuned-conll03-english`**  \n",
    "is a BERT model fine-tuned specifically on this dataset, which explains why it performs well on **formal English news text**.\n",
    "\n",
    "---\n",
    "\n",
    "### How might the choice of tokenizer affect NER performance?\n",
    "\n",
    "The tokenizer directly affects how text is split into tokens, which in turn affects entity recognition.\n",
    "\n",
    "- **Over-splitting words** can make it harder to correctly assign entity labels.\n",
    "- **Cased tokenizers** (that preserve uppercase letters) usually perform better for NER, since capitalization carries important information (e.g. names).\n",
    "- Tokenizers trained on **domain-specific data** (medical, legal, social media) can significantly improve NER performance in those domains.\n",
    "\n",
    "In short, a tokenizer that better matches the **language style and vocabulary** of the task will usually lead to better NER results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61dc70",
   "metadata": {},
   "source": [
    "## Question 4 — Question Answering Systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb38f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631292</td>\n",
       "      <td>335</td>\n",
       "      <td>358</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                   answer\n",
       "0  0.631292    335  358  an exchange of Megatron"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pipeline(\"question-answering\")\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba83dfb",
   "metadata": {},
   "source": [
    "### Question 4.1 What type of question answering is this? (Extractive vs. Generative)\n",
    "\n",
    "This is **extractive question answering**.\n",
    "\n",
    "In extractive QA, the model:\n",
    "- is given a **context** (a passage of text),\n",
    "- selects a **span of text directly from that context** as the answer.\n",
    "\n",
    "It does **not generate new text**; it only extracts what already exists in the input.  \n",
    "This is exactly how the Hugging Face `question-answering` pipeline works by default.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.2 What do the start and end indices represent? Why are they important?\n",
    "\n",
    "The `start` and `end` indices represent:\n",
    "- the **character positions** in the input context\n",
    "- that delimit the extracted answer span.\n",
    "\n",
    "They are important because:\n",
    "- they specify *where* in the original text the answer was found,\n",
    "- they allow precise extraction of the answer,\n",
    "- they make the model’s decision **traceable and interpretable**.\n",
    "\n",
    "In extractive QA, predicting the correct start and end positions is the core learning task.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.3 What is the SQuAD dataset?\n",
    "\n",
    "**SQuAD (Stanford Question Answering Dataset)** is a benchmark dataset for extractive QA.\n",
    "\n",
    "It consists of:\n",
    "- Wikipedia passages (contexts),\n",
    "- human-written questions,\n",
    "- answers that are **exact spans from the context**.\n",
    "\n",
    "The model  \n",
    "`distilbert-base-cased-distilled-squad`  \n",
    "is a DistilBERT model fine-tuned on SQuAD, meaning it is optimized to:\n",
    "- read factual text,\n",
    "- locate short, precise answers within a passage.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.4 Try to think of a question this model CANNOT answer. Why would it fail?\n",
    "\n",
    "Example of a question the model cannot answer:\n",
    "\n",
    "> **\"Why does Bumblebee dislike the Decepticons?\"**\n",
    "\n",
    "It would fail because:\n",
    "- the answer requires **reasoning and background knowledge**,\n",
    "- the context does not explicitly state the reason,\n",
    "- extractive models cannot infer or invent explanations.\n",
    "\n",
    "Another failing example:\n",
    "\n",
    "> **\"What happened after Amazon replied?\"**\n",
    "\n",
    "This information is **not present in the text**, so the model has nothing to extract.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.5 Challenge — Difference between extractive and generative QA\n",
    "\n",
    "| Extractive QA | Generative QA |\n",
    "|---------------|---------------|\n",
    "| Selects answers directly from the context | Generates new answers in natural language |\n",
    "| Answer must exist verbatim in the text | Answer may not appear in the text |\n",
    "| Uses start/end span prediction | Uses text generation |\n",
    "| More reliable and factual | More flexible but can hallucinate |\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.6 Example of a generative QA model\n",
    "\n",
    "An example of a **generative QA model** on Hugging Face is:\n",
    "\n",
    "- `google/flan-t5-base`\n",
    "\n",
    "This model can:\n",
    "- reason over text,\n",
    "- generate full-sentence answers,\n",
    "- answer questions even if the exact wording is not in the context.\n",
    "\n",
    "---\n",
    "\n",
    "### Asking questions the extractive model cannot answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "898fe5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631292</td>\n",
       "      <td>335</td>\n",
       "      <td>358</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.151185</td>\n",
       "      <td>266</td>\n",
       "      <td>302</td>\n",
       "      <td>I hope you can understand my dilemma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                                answer\n",
       "0  0.631292    335  358               an exchange of Megatron\n",
       "1  0.151185    266  302  I hope you can understand my dilemma"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "reader = pipeline(\"question-answering\")\n",
    "\n",
    "# A question answerable from the text\n",
    "question_ok = \"What does the customer want?\"\n",
    "out_ok = reader(question=question_ok, context=text)\n",
    "\n",
    "# A question that requires reasoning or external knowledge\n",
    "question_fail = \"Why does Bumblebee hate the Decepticons?\"\n",
    "out_fail = reader(question=question_fail, context=text)\n",
    "\n",
    "pd.DataFrame([out_ok, out_fail])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9e986",
   "metadata": {},
   "source": [
    "We see that the model always returns a span of text, even when the question cannot truly be answered from the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25319f",
   "metadata": {},
   "source": [
    "## Question 5 — Text Summarization\n",
    "\n",
    "### Question 5.1 What is the difference between extractive and abstractive summarization?\n",
    "\n",
    "- **Extractive summarization** selects sentences or phrases **directly from the original text** and concatenates them to form a summary.  \n",
    "  The summary only contains text that already exists in the document and does not generate new wording.\n",
    "\n",
    "- **Abstractive summarization** generates a **new summary in natural language** by paraphrasing, compressing, or reformulating the original content.  \n",
    "  The summary may use words or sentence structures that do not appear in the source text.\n",
    "\n",
    "The Hugging Face `summarization` pipeline relies on **abstractive summarization models**.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5.2 What is the default model used for summarization?\n",
    "\n",
    "When the summarization pipeline is used without specifying a model, the default model is typically:\n",
    "\n",
    "- **facebook/bart-large-cnn**\n",
    "\n",
    "From the Hugging Face Model Hub:\n",
    "\n",
    "- The model is **abstractive**\n",
    "- It uses the **BART (Bidirectional and Auto-Regressive Transformers)** architecture\n",
    "- It follows an **encoder–decoder (sequence-to-sequence)** design\n",
    "- It was trained on the **CNN/DailyMail** dataset, composed of news articles and human-written summaries\n",
    "\n",
    "This makes the model particularly effective for **news-style summarization**.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5.3 What do the `max_length` and `min_length` parameters control? What happens if `min_length > max_length`?\n",
    "\n",
    "- **max_length** defines the maximum number of tokens allowed in the generated summary  \n",
    "- **min_length** defines the minimum number of tokens the summary must contain  \n",
    "\n",
    "These parameters control the level of **compression versus detail** in the summary.\n",
    "\n",
    "If `min_length` is greater than `max_length`, text generation fails because the constraints are inconsistent and cannot be satisfied simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5.4 What does `clean_up_tokenization_spaces=True` do? Why is it useful for summarization?\n",
    "\n",
    "This parameter removes tokenization artifacts such as:\n",
    "- unnecessary spaces before punctuation\n",
    "- awkward spacing between words\n",
    "\n",
    "It is useful for summarization because summaries are intended to be **human-readable**, and cleaning up spacing improves readability and grammatical quality.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5.5 Challenge — Two different summarization models on the Hub\n",
    "\n",
    "- **Model optimized for short texts (e.g. news articles):**\n",
    "  - **facebook/bart-large-cnn**\n",
    "  - Architecture: BART (encoder–decoder, sequence-to-sequence)\n",
    "  - Training data: CNN/DailyMail\n",
    "  - Best suited for short to medium-length news content\n",
    "\n",
    "- **Model that can handle longer documents:**\n",
    "  - **google/pegasus-arxiv** (or google/pegasus-pubmed)\n",
    "  - Architecture: PEGASUS (encoder–decoder, sequence-to-sequence)\n",
    "  - Training data: ArXiv research papers (or PubMed biomedical articles)\n",
    "  - Designed for long, structured documents\n",
    "\n",
    "**Comparison:**  \n",
    "Both models use encoder–decoder architectures. BART is optimized for journalistic text, while PEGASUS is trained on much longer documents and better captures long-range dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### Why might summarization be more challenging than text classification? What linguistic capabilities does the model need?\n",
    "\n",
    "Summarization is more challenging because it requires the model to:\n",
    "- understand the **global meaning** of a document\n",
    "- identify **important versus secondary information**\n",
    "- model discourse structure and coherence\n",
    "- paraphrase and compress content\n",
    "- generate fluent and grammatically correct natural language\n",
    "\n",
    "Text classification only assigns a label, whereas summarization requires **deep understanding and language generation**, making it a significantly more complex task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac3637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "Your min_length=56 must be inferior than your max_length=45.\n",
      "/Users/alienorsabourdin/NLP-TP2/NLP-TP3/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1633: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (45). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956c766",
   "metadata": {},
   "source": [
    "## Question 6 — Machine Translation\n",
    "\n",
    "### Question 6.1 What is the architecture behind the Helsinki-NLP/opus-mt-en-de model?\n",
    "\n",
    "The model **Helsinki-NLP/opus-mt-en-de** is based on the **MarianMT** architecture.\n",
    "\n",
    "MarianMT is a **transformer-based encoder–decoder (sequence-to-sequence)** neural machine translation architecture.  \n",
    "- The encoder processes the source language (English).\n",
    "- The decoder generates the target language (German).\n",
    "\n",
    "This architecture is specifically optimized for machine translation tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6.2 What does \"OPUS\" stand for?\n",
    "\n",
    "**OPUS** stands for **Open Parallel Corpus**.\n",
    "\n",
    "It is a large, open collection of parallel texts in many languages, gathered from sources such as:\n",
    "- subtitles,\n",
    "- parliamentary proceedings,\n",
    "- news,\n",
    "- official documents.\n",
    "\n",
    "OPUS is widely used to train machine translation models.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6.3 What does \"MT\" stand for?\n",
    "\n",
    "**MT** stands for **Machine Translation**.\n",
    "\n",
    "It indicates that the model is designed to automatically translate text from one language to another.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6.4 How would you find a model to translate from English to French?\n",
    "\n",
    "To find an English-to-French translation model, you can:\n",
    "- search the Hugging Face Model Hub for models with names containing `en-fr` or `en-fr`,\n",
    "- check the translation task documentation.\n",
    "\n",
    "Examples of English-to-French models include:\n",
    "- **Helsinki-NLP/opus-mt-en-fr**  \n",
    "- **facebook/m2m100_418M** (multilingual, supports English–French among many pairs)\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6.5 What is the difference between bilingual and multilingual translation models?\n",
    "\n",
    "- **Bilingual models**\n",
    "  - Trained on a single language pair (e.g. English → German).\n",
    "  - Often achieve higher performance for that specific pair.\n",
    "  - Require one model per language pair.\n",
    "\n",
    "- **Multilingual models**\n",
    "  - Trained on many language pairs within a single model.\n",
    "  - Can translate between multiple languages, sometimes even unseen pairs.\n",
    "  - More flexible, but may perform slightly worse than specialized bilingual models for high-resource languages.\n",
    "\n",
    "**Advantages and disadvantages:**\n",
    "- Bilingual models: higher accuracy, less flexible.\n",
    "- Multilingual models: more scalable, but sometimes less specialized.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6.6 How does the task \"translation_en_to_de\" relate to the model being loaded?\n",
    "\n",
    "The task name **\"translation_en_to_de\"** specifies:\n",
    "- the **source language** (English),\n",
    "- the **target language** (German).\n",
    "\n",
    "This task matches the model **Helsinki-NLP/opus-mt-en-de**, which was trained specifically to translate from English to German.  \n",
    "The pipeline uses this information to apply the correct preprocessing and decoding steps.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6.7 What is the sacremoses warning about? What is this library used for?\n",
    "\n",
    "**sacremoses** is a library used for:\n",
    "- text normalization,\n",
    "- tokenization and detokenization,\n",
    "- handling punctuation and special characters.\n",
    "\n",
    "In MarianMT models, sacremoses is often used to **preprocess and postprocess text** so that translations are cleaner and more consistent with the training data.\n",
    "\n",
    "The warning indicates that this optional dependency is not installed, which may slightly affect text formatting but not the core translation quality.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6.8 Challenge — Find a multilingual translation model\n",
    "\n",
    "An example of a multilingual translation model is:\n",
    "\n",
    "- **facebook/m2m100_418M**\n",
    "\n",
    "This model:\n",
    "- supports **over 100 languages**,\n",
    "- can translate between **thousands of language pairs**,\n",
    "- does not require English as an intermediate language.\n",
    "\n",
    "Another example is:\n",
    "- **facebook/mbart-large-50-many-to-many-mmt**\n",
    "  - supports 50 languages,\n",
    "  - enables many-to-many translation.\n",
    "\n",
    "---\n",
    "\n",
    "### What challenges exist for low-resource languages?\n",
    "\n",
    "Low-resource languages face several challenges:\n",
    "- limited availability of parallel training data,\n",
    "- lack of high-quality written corpora,\n",
    "- dialectal variation and non-standardized spelling,\n",
    "- poorer model performance compared to high-resource languages,\n",
    "- higher risk of translation errors or omissions.\n",
    "\n",
    "These challenges make multilingual models and transfer learning especially important for improving translation quality in low-resource settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48abbea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사랑하는 아마존, 지난 주에 나는 독일에서 온라인 상점에서 Optimus Prime 액션 숫자를 주문했습니다. 불행히도, 패키지를 열었을 때, 나는 나의 공포에 메가트론 액션 숫자를 보냈다는 것을 발견했습니다! Decepticons의 평생 적으로서, 나는 당신이 나의 딜레마를 이해할 수 있기를 바랍니다. 문제를 해결하기 위해, 나는 내가 주문 한 Optimus Prime 숫자에 대한 메가트론 교환을 요구합니다.\n"
     ]
    }
   ],
   "source": [
    "#Here we changed the model since OPUS can't be found currently within the library\n",
    "translator = pipeline(\"translation\", model=\"facebook/m2m100_418M\", src_lang=\"en\", tgt_lang=\"ko\")\n",
    "outputs = translator(text)\n",
    "print(outputs[0][\"translation_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505074a5",
   "metadata": {},
   "source": [
    "## Question 7 — Text Generation\n",
    "\n",
    "### Question 7.1.1 What is the default model used for text generation in the code below?\n",
    "\n",
    "In the Transformers `text-generation` pipeline, if you do not specify a model, it typically loads **GPT-2** by default (often shown as `openai-community/gpt2` in recent hubs). :contentReference[oaicite:0]{index=0}  \n",
    "In practice, you can confirm the exact model name by looking at the pipeline logs/output when the model is loaded.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7.1.2 What architecture does GPT-2 use? (decoder-only, encoder-decoder, or encoder-only?)\n",
    "\n",
    "GPT-2 is a **decoder-only Transformer** (a causal language model). It predicts the next token given previous tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7.1.3 How many parameters does the base GPT-2 model have?\n",
    "\n",
    "The base/small GPT-2 model has **~124M parameters**. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7.1.4 What type of generation does it perform? (autoregressive, non-autoregressive, etc.)\n",
    "\n",
    "GPT-2 performs **autoregressive generation**: it generates text **token by token**, each new token conditioned on the previously generated tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7.2 Why do we use `set_seed(42)` before generation? What would happen without it?\n",
    "\n",
    "`set_seed(42)` sets random seeds (Python `random`, NumPy, and PyTorch if installed) to make results **reproducible**. :contentReference[oaicite:2]{index=2}  \n",
    "Without setting a seed, sampling-based generation can produce **different outputs** each time you run it (even with the same prompt and parameters), because randomness influences which tokens are sampled.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7.3 The code uses `max_length=200`. What other parameters can control text generation?\n",
    "\n",
    "- **temperature**  \n",
    "  Controls randomness by scaling the logits before sampling.  \n",
    "  - Low temperature (< 1): more conservative, more repetitive/deterministic  \n",
    "  - High temperature (> 1): more diverse, but higher risk of incoherence\n",
    "\n",
    "- **top_k**  \n",
    "  Restricts sampling to the **K most probable tokens** at each step.  \n",
    "  - Smaller top_k: safer/more focused output  \n",
    "  - Larger top_k: more variety\n",
    "\n",
    "- **do_sample**  \n",
    "  Controls whether the model **samples** tokens or uses deterministic decoding.  \n",
    "  - `do_sample=False`: typically uses greedy decoding (more deterministic)  \n",
    "  - `do_sample=True`: enables sampling (more diverse, seed matters more)\n",
    "\n",
    "(These are part of the standard text generation controls in Transformers.) :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7.4 What does the truncation warning mean? Why is the input being truncated?\n",
    "\n",
    "The truncation warning means the **input prompt is longer than the model’s maximum supported input length** (context window).  \n",
    "When that happens, the tokenizer/model will **truncate** (cut off) part of the input so it fits the maximum length.\n",
    "\n",
    "This matters because truncation can remove important context, changing what the model “sees” and therefore changing the generated output.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7.5 What does `pad_token_id` being set to `eos_token_id` mean? Why is this necessary for GPT-2?\n",
    "\n",
    "GPT-2 does not have a dedicated padding token by default. During generation (especially in batched settings), the library may need a pad token to align sequences.  \n",
    "Setting `pad_token_id = eos_token_id` tells the model to use the **end-of-sequence token** as padding, which avoids errors/warnings during open-ended generation. :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "Trade-off: using EOS as padding is a practical workaround, but it can be conceptually odd (padding becomes “end of text”), so it’s mainly used for compatibility.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7.6 What are the trade-offs between model size and generation quality?\n",
    "\n",
    "- **Larger models**  \n",
    "  - Typically produce more coherent, fluent, and context-aware text  \n",
    "  - Better at long-range dependencies and instruction following (in modern instruction-tuned variants)  \n",
    "  - Require more compute (slower generation, more memory)\n",
    "\n",
    "- **Smaller models**  \n",
    "  - Faster and cheaper to run  \n",
    "  - More likely to produce repetitive, shallow, or off-topic generations  \n",
    "  - Shorter effective context handling and weaker world knowledge\n",
    "\n",
    "In general: increasing model size often improves quality, but with diminishing returns and higher hardware cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9df9d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42) # Set the seed to get reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c212cc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "Dear Bumblebee, I am sorry to hear that your order was mixed up. I did purchase the right size Optimus Prime figure for me, but I did not receive the correct size shipment. I appreciate your patience as I have been working hard to resolve this issue. I have made many attempts at sorting out this issue, but I have not had a single success.\n",
      "\n",
      "Message for my readers with questions and concerns:\n",
      "\n",
      "Dear Customer,\n",
      "\n",
      "In case you need help sorting out the problem of my order, please look here. I will do my best to answer your questions.\n",
      "\n",
      "Please also consider purchasing the Optimus Prime action figure from your online store.\n",
      "\n",
      "As a lifelong ally of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "\n",
      "Dear Customer,\n",
      "\n",
      "In case you need help sorting out the problem of my order, please look here. I will do my best to answer your questions.\n",
      "\n",
      "Please also consider purchasing the Optimus Prime action figure from your online store.\n",
      "\n",
      "As a lifelong ally of the Decepticons, I hope you can\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1a66d2",
   "metadata": {},
   "source": [
    "-> This output makes sense considering we are using GPT2: a decoder only transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
